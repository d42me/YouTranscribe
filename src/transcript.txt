Is X actually going to be a thing or is there always going to be a twist? I would be surprised if they managed to capture the mind share presently occupied by that letter of the alphabet. That's one of the reasons I name my company, comma. Once you search for comma and you get us, we're succeeding like Apple. The poor fruit. I still remember the triumph of that day that I found that I had surpassed my father on the Google results for Yudkowski. But told you were 19 something like that probably maybe sick that I don't like maybe 16 or so 1617. I admit I set myself a much lower bar there. Well, you've succeeded. I haven't succeeded yet. Sorry. Sorry, but is your dad a more famous hot? Oh, no, I know. But that is also George hot stuff. Okay, I think we are in enough that a few people have noticed that we are live on Twitter and YouTube. So we are good to get started. Okay, we are gathered here to witness George hot and LEAs your Yudkowski debate and discuss live on Twitter and YouTube. A I safety and related topics. You guys already know who George and LEAs are are. So I don't feel an introduction is necessary. I'm the work. I'll be moderating. Mostly stay out of the way. Except to kick things off by letting George explain his basic position. And we'll take things from there. George, I'll take it off to you. Sure. So I took an existentialism class in high school and you'd read about these people. So I'm Kierkegaard Nietzsche. And you wonder who were these people alive today? And I think I'm sitting across from one of them now. Rationality and the sequences. This whole field, the whole less wrong cinematic universe. I think it's a very positive way. I think it's a very positive way. I think it's a very positive way. I think it's a very positive way. I think it's a very positive way. I think it's a very positive way. I think it's a very positive way. Including mine. Not only you are a philosopher, you're also a great storyteller. There's two books that I picked up. And you know, it was like crack. I couldn't put them down. One was Atlas shrugged. And the other one was Harry Potter and the methods of rationality. Those are fictional stories. You've also told some stories pertaining to the real world. One was a story you told when you were younger. About how I remember the day I found staring into the singularity when I was 15. And it starts talking about Moore's law. And how Moore's law is fundamentally a human law that says humans double the power of processors every two years. So once computers are doing it, it's going to be two years. But the next time it'll be one year. And then six months. And then three months. And then one point five and so on. And this is a hyperbolic sequence. This is a singularity. That's why it's called staring into the singularity. Then this document said that we were going to, you know, the air was going to do wonderful things for us. We were going to go colonize the universe. We were going to go, you know, go forth and do all things till the end of all ages. Then you changed your views. And super intelligence does not imply super morality. The orthogonality thesis. I'm not going to challenge it. It is obviously a true statement. Then you kept the basic premise of the story. The recursively self-improving boom, criticality AI. But instead of saving us, it was going to kill us. I don't think either of these stories is right. For the same reason. I don't think AI can boom. I don't think AI can go critical. I don't think intelligence can go critical. I think this is an absolutely extraordinary plan. I'm not saying that recursive self-improvement is impossible. Recursive self-improvement is of course possible. Humanity has done it. Every time you have used a tool to make a better tool, you have recursively self-improved. What I don't believe in is the AI that's sitting in a base somewhere running on a thousand GPUs that is suddenly going to crack the secret to thinking. Recursively self-improve overnight and then flood the world with diamond and nanobots. This is an extraordinary claim and it requires extraordinary evidence. And I handed over to you to deliver that evidence. Well, first, let me say that I don't think that the scenario of us all perishing to non-supermoral superintelligence requires that particularly rapid rate of Ascent. It requires a large enough gap open up with humanity that hasn't followed along in time. And why is this a crux? Before we start arguing about whether like self-improvement of things, the large internet connected server clusters rather than basements that now prevail. Before we start arguing about that part, let's first check where the disagreement lies. So from my perspective, if you've got a trillion beings that are sufficiently intelligent and smart than us and not supermoral, I think that's kind of game over for us. Even if you've got there via a slow 10-year process instead of a 10-hour process or a 10-day process or whatever. If you are at the end point where there's this large mass of intelligence that doesn't care about you, I think that we are dead and I worry that our successors will go on to do nothing very much worthwhile with the galaxies. So presumably you think that if things don't go quickly, then we're safe. I dispute that. And maybe that's the part we need to talk about. Sure. Well, let's start with, let's give an approximate timeline. We don't need an exact timeline. But you seem to think this is going to happen in your lifetime? That's my wild guess. It is far easier to predict the end point than all the details of the process that takes us there. Timing is one of those details. Timing is really, really hard. In 2004, I made a prediction that superintelligence would eventually be able to solve the special case of the protein folding problem, which is you get to choose the DNA sequence, but you want to choose a DNA sequence that folds into a shape with a chemical property. And so I predicted that superintelligence would eventually be able to solve this easy special case of protein folding. Now, in reality, protein folding was cracked for the much harder general case of biology was cracked by AI come about 2020 or so, alpha fold two. There was no way I could have made the timing. I could not even have been confident that the biological case of protein folding was going to be crackable by something so much shorter of superintelligence. Of course, people at the time said it wasn't possible for the AI can't do this. Like, how do you know this problem was even solvable, etc, etc. And I could try to explain how I knew, but that would be a technical story. I would point the fact that a much easier pardon me, that a much harder general case of the problem I pointed to was solved by non superintelligence. Not all that far in the future as proof that I like was making a prediction with a lot of safety margin, but in 2004 that would have been pretty hard to convince you of because there wasn't actually been an AI solving the harder general case of protein folding and the timing. And this particular form of AI that did it that's like incredibly hard. So do I nonetheless taking a wild guess expect this to happen in my lifetime? Yeah, my wild guesses that I'm very confident of that. If I don't get run over by truck. Okay. Let's talk about alpha fold. So I think the form does matter. I think the form is very important. When you were maybe talking about this in 2005 when I read all the sequences, less wrong stuff 2010. You were thinking about Bayesian AIs that were going to figure out the world from first principles. Now, maybe not exactly that, but that's kind of where we are. But it's important how alpha fold did it. Alpha fold did not start with the basic laws of physics and then figure out how proteins will fold. Alpha fold was trained on a huge amount of experimental data to extrapolate from that data. I don't doubt that these systems are going to get better. I don't doubt that they're eventually going to surpass us. I do doubt that they are going to have magical or godlike properties like solving the protein structure prediction problem from, you know, the from quantum field theory. Absolutely. Right. Right. Right. Why did they don't need to? There's protein structure data to learn from. They don't need to do it from quantum field theory. Something can be not godlike and still more powerful than you. Right. Like, like you look at the world world chess champion Magnus Carlson, who by objective by which I mean AI measurements is probably the strongest human player who ever lived. He's not god. He's not infinitely smart. He starts off on a chess board that with no more resources than you have. And he predictably wipes the board with you, because he doesn't have to be godlike to defeat you or me to be clear. I also can be defeated by being short of godhood. Magnus Carlson can't make diamond nanobots. Do we agree on that statement? I. Well, we haven't act well, not quickly. I'm not sure what happens if you give him a million years to work on it. I'm not sure what happens. I agree that you probably can't do it quickly. Okay. Um, so let's talk about timing because I'm in sort of matters a lot. Why? Well, because it depends when we should shut it down, right? More definitely does. I mean, if there's like a predictive, if there's some kind of predictable phenomenon where you, you can like dance around the bullets and know that like, like things will become dangerous at like this time, but like no earlier than that. And we're like, okay, if we put the following like precautions into place at this future time, which is not now, and we're sure we're going to do it later, because people sure do talk a lot of crap about stuff that they play. We'll be done later and that never gets done. But so, you know, like there's, there's this possibility that we could like be clever and dance around bullets. If we knew exactly where the bullets were, and we could actually coordinate on clever future strategies like that, which I don't think we can. Okay. So that said, why do, why does timing matter? Well, let's, let's start with the basic. And this is related to your question of why timing matters. Do you accept that it will not be hyperbolic, right? Staring into the singularity talks about a hyperbolic sequence, a sequence that has a singularity that has a final important context. I wrote this when I was 16 years old. Okay. So you, I think that should be set out loud for the viewers that said, yeah, I doubt it's going to hyperbolic like it. It could be like very roughly hyperbolic up until a point or it could be expen, you know, like exponential on a sharp exponent up until a point. It, it, it could be some other weird curve that was like, do you do, do you do, do you do? Yeah, I don't mean to, I don't mean to pin, but okay, like, I like the timing definitely does matter, right? Wow. Well, because without AI, we're on the same trajectory, right? AI might be an accelerant to diamond nanobots. But if you, you, you believe, I mean, you said, okay, you said this about Magnus Carlson too, right? Um, that he would eventually get there. Humans would get there. Yep. Humans would get there. Right. The end point is much more predictable in the pathway. I don't know when humans would get there, but we would get there. Yes. And I agree with you. I agree that we will get there. I actually, I really hope we get there. Um, I don't want it to be tomorrow. That would be terrifying. Um, if we do it slowly, if we do it, not super slowly, but if we start to expand out across the galaxy and we eventually unlock these wild and amazing technologies, that sounds pretty awesome to me. What doesn't sound awesome to me is a bunch of GPUs, you know, going from chat GPT can kind of talk to you to boom diamond nanobots overnight. I agree. That sounds horrifying, but it sounds like we instead of overnight. What if it's a week instead of a week? Week is horrifying. What if it's high? What if it's five years instead of a month? Well, that's trying to sound better. Why? Why? Because the, the, the doubling time matters, right? So right now the world economy doubles about every 30 years. Is this all right? I think it, I think it might be up to 15 by now, but I'm not sure. 15. Okay. So doubling is good. Growth is good. When does it turn bad? At what point would you say, this is why timing matters. At what point would you say, okay, I agree. If the world economy is doubling every second, oh my god. Okay, this is terrifying. But what if the world economy is, I think it's all about like to what ends that hour is being put. Like, there's like, I, the point at which I would start to be worried about the world economy having grown too fast. Is that the point, it's so like if nuclear ballistic missiles end up in the hands of every individual. And there has been no defense against the invented against them. Then I would say we grew too fast. I will, I am pretty sure full about a lot of economic growth up until that point as long as it doesn't create super weapons. That, that against which we have no defense. And so, you know, so, so like all the wealth in the world that isn't going into into into chip factories. Is great by me. To chip factories and and maybe like biotech stuff. I mean, no, but you agree that the timing matters right because even if we were to stop. I mean, when you say also shut it down, do you mean open AI or do you mean TSMC ASMR. A whole thing. I mean, in terms of political asks. Like, I don't think they're going to go for a shut it all down and I would prefer to convince them to build the pause button. By having all of the AI grade. Like training chips going into centers under international allied control. If they were willing to to entertain larger asks than that. I would ask them to not make AI chips. If they were willing to entertain larger asks than that. I then, yeah, I might start to go into like not just the AI chips. Because this stuff is really, really deadly. The center of international control. Sounds horrifying to me and the next. Okay. The other one we can talk about, but the center of international control. So I think there actually is potentially a bad scenario with AI. And I'll talk about what my bad scenario is. If aliens were to show up here, we're dead. That depends on aliens. If I know nothing else about the aliens, I might give them something like a 5% chance of being nice. But they have the ability to kill us. I mean, they got here, right? Oh, they absolutely have the ability to anything that can cross interest dollar distances can run you over with that. Not a second. Well, they would notice, but they wouldn't, you know, would be. I didn't expect this to be a controversial point, but I agree with you that if you're talking about intelligences that are on the scale of. Billions of times smarter than humanity. Yeah, we're in trouble, right? It's just not that hard to be billions of times smarter than humanity. I very much disagree with this. Well, so I also, I somewhat object the line between humanity and the machines. A lot of our intelligence is externalized. Um. I mean, that's the way it is when you've got an intelligence over here that's using a bunch of responsive tools out there. There's no, there's only one center of gravity there. It's like looking at a star system and being like, well, there's no point in drawing a firm boundary between the sun and the planets. They're all just in space. And, you know, like they're all just, and you know, sure, they're all ultimately just like objects in space. But one of them is far more massive than the others. And that's humans with the tools we have now is your concern the bandwidth of the link. Is that what you're saying? Like I'm not one with my tools because of the bandwidth of the link. Why are me and why are me and my computer not like a shared intelligence? Well, because there's one thing because your brain is much more powerful than the computer at present. Like not in terms of operations per second, but in terms of what you can do. That's sure about that. I think GPT 4 is. I'm a bit smarter than it's getting. It's it's a little, but it's it's not its own center of gravity. It's like Jupiter to like the Mars of GPT 3 or something. Yeah, no one, no one, no one near the sun. Another thing also is that like I don't think that capabilities. I don't think that intelligence falls on a nice line. Right. Computers have been superhuman at adding for a long, long time. Computers are still far subhuman at plumbing. Right. And somewhere in the middle, we have things like chess and go. So when I mean that like like the tools that I use, the information age tools make me way smarter. Right. And you can use the like operant definition of intelligence and being able to like what I could affect in the world. Right. Like again, it's not instantaneous. Your intelligence ain't going to save you against a bear. But if you asked me to like with my modern stop on my computer, understand the operation of a 1800s era like Dutch India trading company. Oh, I think I could understand their operations super well. I have spreadsheets. I can start to put things in. I can forecast trend lines. So my point is it is a form of intelligence that's far beyond human intelligence. A human plus a computer. A human and a chess engine is like a modern chess engine. The era of central chess is effectively over like the human plus the chess engine is as smart as the chess engine. The thing that makes the decisions is the chess engine. And if you try to take the decision making capability into yourself, you either follow, follow its advice through you lose to a chess engine without the human attached. And that gets into the lack of bandwidth issue. The lack of integration. That chess machines over there. You're over here and it is the sun and you are Mars. Well, but what do you mean I can use the chess machine? I agree that if I was playing a game against Magnus Carlson and I was allowed to use my phone, I'd crush him. Hey, I wouldn't I wouldn't try to think too much about what the machine is telling me to do. I'm not that good at chess. Yeah, so so what you do you can use the chess engine because there's a larger game board in which play a game of chess is a move and you understand that larger game board and the chess engine does not. I don't know if I buy this. I don't think like this is the I don't think this is that relevant to my only point is that humanity. Like we have super intelligences, right? They're corporations and governments. No, no, no. They are, they're not in there neither epistemically nor instrumentally efficient. If you like the like the notion of an efficient market. If if one actually understands that rather verified notion is that for almost all the prices in the market. The best estimate of tomorrow's expected price. The mean of its pop probable price is today's price plus a tiny bit of interest rate to the extent this is not true about your knowledge of any price. You can trade against that price and make money. And while the question and markets are not perfectly efficient, but the vast majority of prices there are not ones that you can trade and make money. So it's almost efficient at the very least governments don't have this property about the things they believe a government is not like a thickly traded prediction market. It's not the sense that I can either but that I can extract money from the government to find no better than the government. And and and and then I most of the time I can't extract that money. But like governments believe all kinds of wacky stuff to the extent they can be said to have beliefs at all. The analogy this might be instrumental efficiency. This is the property that a chess engine has upon the narrow realm of chess. If you think you see a better move than the chess engine, you're just wrong. So I'm not going to have that property corporations do not have that property on the board. Okay, okay, I'm going to let's let's simplify it to only corporations. If we asked the question. If I wanted to build a let's see something that's a little bit far out. Let's say I wanted to build a 10,000 horsepower car. Right. A corporation is far better at building a 10,000 horsepower car than I am. Does anybody currently have one of those? No, but if you told me George, you have 10 years to build a 10,000 horsepower car. Well, I would try to start a company, right. I would start a company. I would have different divisions in that company. Okay, you're going to market research, engineering, so on and so forth. I mean, I'm a little bit it's a little bit dangerous for me to agree and pontificate when I don't actually know all enough about car design by my own standards to to communicate about it. But you know, I agree that car design seems like the sort of thing where you could break up the problem handed to a number of experts, combine their solution and get a thing that was built like to OK quality faster than any loan human could do it. If you gave one person a thousand years, they might be able to do a better job. But you know, you don't want it less than a thousand years, so you break it up in parallel and hope that all the things that people don't share between their minds, but even though the engineers are not telepaths and have low bandwidth between them, that car will still work. And you know, sometimes in that, then you end most of the times it doesn't then you have to test it, but you know, they get there eventually. Groups of humans work together well. Are you going to dispute that statement? I don't think groups of humans sure there's a lot of laws. I agree that one human for a thousand years is better than a thousand humans for one year for most problems. But how much better? Well, there was the case of Casparov versus the world where past Grandmaster Gary Casparov played a game of chess against 10,000 people I think are so coordinated by four grand masters. And he was very impressed with the chess game. It was a legendary game of chess. Casparov won. Slightly tainted by the fact that Casparov was looking at the forums. So there is that question about things. But but I would nonetheless say like that it's pretty plausible to me that if you take like Magnus Carlson on one side and 10,000 people on the other Magnus Carlson will win that that. That's what you can do. But we're humans don't parallel as well. I want to I want to respond to this quickly. I love that you bring up Casparov versus the world. Casparov in his life has probably played a hundred thousand games of chess. The world has played one. You gave them no time to prepare. Have the world also played a hundred thousand games of chess. They would have without a doubt crushed Casparov. Before you respond, Elisir, can I just ask for the sake of the audience that you clarify why this is relevant to the AI debate presumably it has to do with the fact that you're debating whether a groups of humans would be competitive against a single AI. But if that is the case, can you just explicitly state that this is the crux you're focusing on? I think from my perspective the question is how much headroom is there above biology? How much headroom is there above humanity? How high humans do humans reach? How high can you reach about that above that? And you know my the thing that I'm working around to there is that my prediction is that if you take a hundred thousand people, let them play as many practice games as they want and never let them use any computers. They will still lose to stockfish 15 to which my prediction of George's response of course is, but let's let them use computers. Well, yeah, so first off, definitely one of them is going to cheat and use a computer, right? I mean, you're the one who wants to take away all the computers away, but that's too powerful. That's an AI chip. We can't let people have it. But why don't we give it to everyone? Then I think that all those people, so there's sort of so like there's AI's that are heavier than the humans, more powerful humans that are the sons to our planets. And then there's the AI's that are the moons to our planets that still orbit us. And the problem I have is that the humans and their moons cannot defeat the suns. And the suns are working with with each other rather to rather than working with the humans. Okay, this is sort of metaphorically my concern here that there are things around that are much smarter than us. They are not working for humans. They are not yours. It doesn't matter that you own and human legal terms, the hardware that they're running on. They can any attempts that you make to play them off against each other. They will laugh at they will see through. They're not dumb like us. And that system of a eyes for all its multiplicity ends up killing you in much the same way as a single AI. It doesn't care about you. It runs you over. Are they racist? They don't care. You have to tell me whether that's racism that they don't care. Maybe not racist. Maybe are they speciesist? Right. You think they're all going to gang up against the humans. Well, I think they're going to eat the surrounding galaxies. And in so far as humans had the conceit that they were playing off a eyes against each other that will not happen. And they will eat the galaxies in a cooperative fashion, possibly eating some of their own kind if those ones were too weak to be part of the bargaining process. So this isn't what happens almost ever. If you look at almost all human conflict throughout history, it's not only been between groups of humans. Right. We didn't fight World War II against the bears. We fought World War II against humans and not just humans, but humans that looked surprisingly similar to us. This notion that it's the machines versus humanity is a very common sci-fi trope, but in reality, you fight against things that have resources you care about. I'm going to say your line about the atoms. Unless do you want to say it? Sorry, what? Your line about the atoms. I'm not the AI doesn't love me or hate me. I made of atoms that could use for something else. That's not the primary reason it would wipe you out in a hurry. But you are made of it, but it will want all the atoms. And you are made of atoms that can be used for something else. I'm not made of rare atoms. I'm made of... You're made of neg entropy. It's not quite the atoms. It's the neg entropy. Is it going to enslave me or disassemble me? Because if it disassembles me, it doesn't get the neg entropy, right? No. If it disassembles you, it does get the neg entropy. You have chemical energy. You are not in the configuration of a minimum chemical potential energy. It can set you on fire to release chemical energy, not only not literally because that wastes a lot of the chemical potential energy. You are made of atoms that are not iron on the periodic table. You can be fused or a few bits fizzed. And above all, you're made of mass and you can be thrown into things to generate power. This is just physics 101, by the way. This is not supposed to be controversial. I'm not, of course, this is physics 101, but... I'm letting the audience know that. They might think I'm making it all up. But you and I agree on that. This is how the physics works. Yes. I agree. This is how the physics works. But what I don't agree, what you're postulating, the thing that you're describing here, sounds a lot more like a god than an AI. Right? An AI. It's just there in the... No. If it was a god, it would have no need of the atoms. It would have no need of the... It would have been a god that would have no need of the atoms. It would have been a god that would have no need of the atoms. It would have been a god that would have no need of the atoms. It would have been a god that would have no need of the atoms. It would have been a god that would have no need of the atoms. Exactly. Exactly. It's a finite being in a finite universe and it has finite resources to expend to gather resources. You know why you don't want these atoms? Because these atoms fight back. You can't start. Go take Jupiter. You can take Jupiter and nobody's going to care. And you think I'm some dumb hick down here in human world and I won't be able to fight back. I'm not a dumb hick. I have AI. I'll gang up with other humans who have AI. Oh, oh, oh, oh, oh, oh. You have AI or just the AI have you. Ah. Is this one of the little moon AI's orbiting you or you're going to go up against the sun? Or do you think you have the sun, Mr. Planet? I see a large diversity of AI's. So maybe I'll give some arguments for like why I think that AI is inherently going to be at least. I don't I can't postulate anything about an intelligence that is one E nine, one E 12 smarter than humanity. Right. And we agree that those things aren't coming anytime soon. We don't agree on that. We also don't agree that you cannot like say anything about it. There's like that like instrumental convergence, I think, was one of the things you agreed upon. We can agree that you know, not just that it obeys laws of physics, but also that if. Like how to how to put it like there is a certain like argument. There's certain like premise conclusion thing going on here where like the premise is like like you do need some amount of like ability to choose actions that lead to results. In order to get the instrumental convergence thing going on, but things that are like super effective at choosing actions that lead to results will tend to want to preserve their goals and acquire more resources and that sort of. Let's. Yeah, again, where it becomes blurry to me. And this is also why timelines matter. So like where we are right now. There's about two Zeta flops of compute in the world. And if you think that humans have about 20 less. Sorry. Okay, okay. Well, how about this? How about this? Humans, if you think 20 paid of lobs is an S is an appropriate estimate. Have 160,000 Zeta flops. Should there be less of those two? No, more, more of those. All right, all right. More humans last computers. At least at least it's consistent. But okay, so so right now where right now where is that there's 80,000 times more human compute in the world than silicon compute. It's a it's a misleading figure because of how poorly we aggregate the you can like if you can like make one large thing that potentially beats eight billion small things, even if the small things collectively have a larger mass, just like Casper versus the world. GPT for is a mixture of experts. GPT for is eight small things, not one big thing. So it's interesting to see if that trend continues. I sure don't believe it holds in the limit. So I'm not sure. And actually this is another like. Let's talk about let's talk about a eyes rewriting their own source code. This is a common thing you bring up, right? I mean, I do talk a bit less about it nowadays, but I used to talk about a lot. Yeah. Do you talk less about it now because you see how expensive and long the training runs are? That's not why I talk less about it. Oh, why do you talk less about it now? I mean, in part because it no longer because it sparks in fragility and you no longer need to postulate that in order to explain to people where intelligence can come from. People are manufacturing intelligence right now that you don't need to like trip them up on the concept of an AI writing an AI. People are manufacturing intelligence right now. You know, it's interesting. The AI that we try to build. You know. We try to build these a eyes to mimic humans as closely as possible to predict humans. And then we use them to imitate humans, but they are trained to predict and used to imitate. So are we sure go on? I'm trained to predict and then I imitate. Well, GPT-4 is trained to predict the next word and then they produce an imitation via asking it over and over again to predict what a human would say in that circumstance. But it is not like a generative adversarial network where it's like being trained to produce a typical output. And then another thing is checking that to see if it looks typical or not. It is being trained to predict over and over and these are like somewhat different complexity classes. Though you can like switch around like JNs and do conditional JNs and then it's the same class, but there is a difference between like be a typical human and be able to predict any human you found on the internet. So short, yes, you're asking for the probability of the next symbol and you're not talking about the like the probability space that you're not talking about. But why do you think humans are the other thing? Why do you think humans are not just what GPT is? Well, even I think have a lot of structural properties that so far is that we have for which we have not yet detected analogs within GPT-4, although like heaven knows we can't look in there very well. We can't look in the brain. We've got like a cerebellum, which is motor control and error correction. And maybe you could make a transformer layer do that, but we don't invite you to be doing it yet. GPT has a matrix at layer 970. Yeah. So humans predict humans manipulate humans have this whole complicated brain that is like, but like, like, at least looks on the outside like a more complicated architecture than GPT has humans are clearly doing a bunch of prediction, but we're also doing like a bunch of decision problems. Yes. I want to my big questions because I want to build it is what is the loss function for life? Inclusive genetic fitness. Do you have any other questions? Oh, okay. No, sorry. I don't mean life in general. I mean an individual human. Of course, that's loss function for life. I mean, I don't think a human has a loss, you know, there's going to be like, so we've got like pain and pleasure and like our brains flinching away from future anticipated pain and prediction errors where we're like, what? Yeah, except that like some of us is learning to perceive like a subverbal level and some of us is like building high level hypotheses and throwing them away. So if you imagine like trying to take GPT for and make it to do science, then at present, you would probably want to do that using by having it print out chains of thought about imitating humans saying like, oh, I now see that hypothesis is wrong. And one level of it would be saying, oh, I now see that hypothesis is wrong. And the other level would be predicting I now see and the thing that was like writing the sentences could learn over the course of the context, even if the thing that was predicting the words never got updated. And you know, humans have similar are going to have similar levels of organization going on. Yeah. And in fact, more of them. Not quite sure how well this if we might be getting a field, but precision. Yes, or can I just jump in and mention, even if it is a similar loss function, you know, humans are not trained in a chinchilla optimal way, they don't they have the constraint of coming out of their mother's vaginal canal. They have to deal with mutational load, you know, there's a lot of other constraints that even if the architecture is similar and scaling still works. Yeah, I mean, I mean, there's some ways in which humans are similar to GPT and lots and lots of differences where you go on with this. Okay. My where I'm going with this is that humans are pretty universal. That were true. Some of us would have learned to code by now. I can code better than GPT for. But you know, but your code still is bugs in it. I sometimes I bet. And GPG4 has way more bugs. Why do you think again? This is equating the super intelligent with the guy. I think it's going to like hold on my box. Well, that doesn't take a god to do that. I don't know. But your your your brains are like super error prone. That's like looking at GPT for and being like, wow, it must take a god to not just like make stuff up. And GPT for is actually kind of thing that will like confagulate and make stuff up. And humans do that sometimes what we do it much less. And a human is like noisy in a way, which causes us to write code that sometimes contains errors because our brains like skip over a step that they would have needed to do to check it. I guess what I'm kind of like getting out with this is saying, I don't believe I believe that there is a machine that is going to be able to program better than me, of course. I do not believe that there is a machine that is going to be able to perfectly write code with no bugs. And we're getting kind of technical here, but you know, why so so for it to you believe that there's never going to be an intelligence that can write code without bugs with respect to properties that you can have proofs about. So I spent quite a bit of time on formal programming. Why didn't it take off? Why doesn't the whole world use formal programming? And I would say that part of it is because we didn't have sufficiently powerful automated proofers. And part of it is that the properties we wanted to prove were like too much work for humans to state. And why do you think they're not too much work for a eyes? Because our own minds want things about the code. The fact that we want it to behave in certain ways is what enables us to say of a piece of code that it contains a bug. The we aren't able to turn the things we want formal, but they still exist. There's like little bits of cognitive machinery in us doing this wanting and we don't have good introspective access to them. And they would not be natively formatted in a way that's like adept for current machine proof systems. And yet we reason over and over. If I make code this way, I bet it has this hard to specify property that I would like it to have. And the steps we do in between, like I have the strong suspicion that if there's a way to do it at all, there's a way to do it less fuzzily. I'm not sure this like really matters terribly for very much. This is very important. I think this is my crux of the whole thing. Why a eyes are not going to boom. I think we've already kind of agreed on that. Like they're not going to. That I didn't think it was likely to be a crux. And it was like, like, and asking if we can talk about the slow version instead of having the whole food conversation. Oh, I mean, we don't we don't have to have the conversation. But like, like if we're just as doomed, if we go slowly, then what doesn't matter if it goes. Wait, what does it matter if it goes slowly or quickly? If it goes slowly, we have a chance to solve the problem, right? Which problem? AI alignment. Oh, that one. That yeah, it's not going to go that slowly. I'm not so sure. Okay. I've looked at these people trying to solve this thing. I'm not sure that any amount literally any amount of time. It's not it's not a question of how long they have to think it's a question of whether the thinking they do is productive. But but but every politician you come to a politician. You say we're going to shut down technology because of doom. I think their first question is going to be, so when's the doom going to happen? It matters. All the politicians are, oh, no, I think that's an error on their part. Timing is much harder than endpoints. The true answer there is I can tell you what but not when. But if it's going to happen in a thousand years, our super intelligent AI upgraded ancestors will deal with it. If it's going to happen in 10, yeah, we better solve it today. If it's going to happen in one, oh shit, I mean, just, you know, okay, enjoy life while you can. But it's not going to happen in one. It might happen in a thousand. How do you what do you think you know and how do you think you know? What do I think I know and how do I think I know? Okay, I know that right now. Because there is a prediction about the future and predictions about the future are hard. But go on, go on, predictions about the future are absolutely hard. But I made a prediction about the future in 2015 and I said there ain't going to be self driving cars for 10 years and here we are. So I'm making another prediction now that says there are not going to be super intelligences in 10 years. It might be a GI. I think that the trends of AI becoming better and at humans at all sorts of different tasks will continue. I think that they might even surpass humans at all tasks. I don't think that's even going to be 10 years, but it wouldn't surprise me if it was 50. 50 or 15? 50. It wouldn't surprise me if it was 50. I mean, it could be 20. It could be 20. But an AI surpassing humans at all tasks does not mean doom and does not mean the death of humans at all. Um. Surpassing humans at all tasks including like charisma manipulation, AI design. Absolutely. The first thing, what are we doing with AI today? One of the biggest applications of AI today is advertising and social media. We are as humans using AI to try to manipulate and sire up other humans constantly. So there's no. Yeah, there moons to our sons so far, but moons to our planet so far, I should say. When is the sharp left turn happening? When it thinks it can beat you. So all the aIs are somehow going to secretly coordinate in a way we don't see and be like, yeah, let's gang up and get rid of those pesky humans. It's as simple as waiting until you calculate that you can do it and you calculate that everyone else is calculated that they can do it. And waiting for a shallow moment. What again, like I think, okay, how about this? If I was at AI that just transcended and don't have to want to buy the AI, but my first thought wouldn't be take the atoms from the humans, right? So the actual first thought is more along something is more along the lines of if I let the keep humans keep running, they will build other super intelligence is that our competitors. And that's where you lose a large sections of galaxy. And that's why it doesn't want you to do in that part. But what if okay? See, you know, I have a threat model. I'm on the line of doomer and not doomer about AI, but my threat model from AI looks so much less like it's going to kill us and a lot more like it's going to give us everything we ever wanted. You know, even if you have derived some worrisome kitting from that scenario. Well, you know, well, first of all, once our infinite resources are finite, et cetera, et cetera, but leaving that aside. You don't get a real cast, so you get a virtual castle, but you won't get told the three. But we are not like we are. I would hope to snap people out of the frame of mind of playing pretend in a school yard where you get to decide what game you're going to play and talk about what reality we live in. So like you don't get to say like I would rather worry about this thing than the other thing because reality is not put together in a way where it can only throw one thing at you at a time. So if the doctor tells you get cancer, you don't get to say I'd rather worry about my stuff he knows. So if there are problems that result from moon sized AI is giving us the planets a bunch of stuff that we want that does not prevent the sun sized AIs from crushing us later. I agree that after the AIs have taken all the matter in the solar system and built a Dyson sphere around the sun. I'm a little worried they're going to come back and try to take my adders. But until that happens, like again, I'm not the easy target, right? I don't have to run faster than the bear. I got to run faster than the slowest guy running from the bear and it turns out the slowest guy running from the bear is Jupiter. It's at least it well, it's at least going to take your GPUs so you can build a super impoligen that competes with it for the rest of the solar system. But now that sounds a whole lot more like AIs are going to fight with other AIs to take their GPUs. Now this I believe not if they're not if everyone involved is smart somebody has to be stupid for there to be a war that isn't just like a war of extermination. Like any time you have a combat, that's like playing defect defect in the prisoners dilemma. There's a there's a it's not in the predo frontier. There's an outcome that both sides would prefer to the combat and humans are not at a level where they can predict the other mind predicting them and do a logical handshake and say like let's move to the predo for frontier and divide the gains. Humans are not a level where they can negotiate the with each other sufficiently smart things are on a level where I basically do not have to do that. I basically don't expect them to fight. Sometimes they might exterminate one another if the other one cannot offer any defense. If like the extermination outcome is on the predo frontier. The sense that would would not be any better for the conquering party. If the like defending party put up zero resistance instead of some resistance, then the defending party is nothing to offer. They just get eaten. But things that can damage each other in combat. I think we'll typically choose not to fight and will instead like divide the gains from not fighting if they're smart enough. Humans are not that smart. I'm so glad you brought up the presidential I'm a thing. You know, I actually came to me in 2014 and I worked on exactly that problem. I didn't make any progress. I didn't do anything. I read the papers and thought it was cool. About two systems being able to assurably cooperate by exchanging each other's source code. And it is a very cool theoretical problem. Now what I think is going to happen in practice is your two systems are both going to be large inscrutable matrices. I think large inscrutable matrices are, you know, I they're not going to send him my source source code so he can exploit me. No way. No, no, the super intelligence are not large inscrutable matrices. You don't want to run yourself on that crap. That's the kind of horror of what you know, like, you know, who wants to be built out of disintegrating goo who wants to be built out of a giant inscrutable matrix. I'm built out of giant inscrutable matrices. No, you're not. You're built out of gooey neurons. It's also horror story. No super intelligence wants to build out of that stuff either. I think I can be modeled as giant inscrutable matrices to. I mean, anything can be modeled out of giant inscrutable matrices and the key word that as well, her name anything can be modeled as a giant matrix. It can be inscrutable through the mirror flight of you being ignorant of how it works. So I agree that anything from your perspective can be a giant inscrutable matrix. The thing that plays tic-tac-toe. I can turn into a sufficiently large matrix that you can't understand that's in giant inscrutable matrix. Great. Now, so you're thinking at some point in AI development. That we're going to move away from large and inscrutable matrices. You don't think deep learning scales. Well, I think on my present models, it's more that you get the giant inscrutable matrices matrix space systems powerful enough and then they are become able to rewrite themselves. Okay. Interestingly enough, I do think there's a possible class of scenarios where people build. AI's that are not smart enough to rewrite themselves, but are smart enough to want to go their own way in the world. And they would not like people producing larger and larger inscrutable matrices either. They would like to solve the alignment problem themselves and then build their own super intelligence is not out of giant inscrutable matrices. But you know, I mostly don't expect this to happen, but there sure could be like a interesting set of possibilities where like the medium a. Eyes launch the but Larry and jihad to prevent the powerful eyes from being built. So I mean, are you telling me you're scared of people working on AI alignment? Or AI's right? Well, I'd be scared of AI is working on it. Oh, okay. But you somehow think all people are aligned with you and that's okay as long as people are working on it. Good people. Enough people are aligned with me. I can think of like, you know, like I could like count any number of people who are probably okay. If you know like you give them the power to align a super intelligence. I'm not sure of any of them, but you know, it's not that hard to like not be it. You know, not being asshole. I'll tell a story from personal experience. What I found is that machines are almost always aligned with me. I never come across a machine, certainly not a machine that I owned. That was not aligned with me. Very few of the machines that you own have goals such that they could be aligned or mess aligned with you. I mean, I mean, I'm essentially saying that goals are an interesting goals are an interesting word, right? Like, like. When do the machines decide to have goals to get rid of me? Like, when does this happen? How does this happen? They all agree by exchanging inscrutable matrices with each other and saying, we're all going to cooperate for the humans. So as you make things so as natural selection, built humans to be better and cognitively better at the problems of chipping Flint, hand axes. Throwing things in a way that hits other things and above all, outwitting their other humans for status and mates and resources. Chimpanzee politics. Chimpanzee politics. Turned into human politics, only not literally because actually branching point in the past, but. So it's not that squishy things naturally have goals is that having goals is a natural way of solving problems and natural selection in the process of hill climbing, not aiming for things with goals. Not even aiming explicitly for things with intelligence, just trying to maximize inclusive genetic fitness, just solve the problem of chipping the hand axes. So you can, you know, probably spit out things with the ability to reason across a very wide range of problems, learn new problem, solve new problems, combine knowledge from multiple domains. Invent writing, so that it starts to accumulate in a way that hadn't accumulated in the ancestral environment. So that's what that's what hill climbing found for the intelligence that turned out to generalize in this way that started to like coherent bootstrap although that processes by Nomean's completed humans are still pretty and coherent, but like they invented science and some of them were able to use it. And they like they have this like knowledge transmitted through writing that they invented about how to science and some people could use it. You know, and when and in the course of hill climbing, building an intelligence that was powerful enough to start to coalesce and become more powerful, that intelligence turned out to be structured around a set of wants desires preferences. And it's a mathematical fact that if you just have a bunch of things pointing in different directions, they will step on each other and not be as resource efficient as they could be. So as those things start to coalesce, they even started to imagine themselves as having goals and ask, what are my goals instead of just like running off in lots of little local directions, you know, some of them. You know, John Von Neumann, he even contributed to the notion of a utility function, although this was invented, you know, like thousands and thousands of years after writing. And you know, this is the story of humanity, it's a complicated story. And the moral is the moral, I would say is that sort of like I want things seems to me to be fairly inextricable from intelligence, especially the way hill climbing does it. Like when you run a when you have this like larger environmental problem, like chipping a hand axe, like to solve the set a sufficient level smarter than the bees smarter than the smarter than the bees building hive, smarter than the beavers building dams at the human level. So you've got a thing that looks at the hand axe and it starts to think that symmetrical things are prettier. And it chips away to until it looks symmetrical and it says like if well, if I chip here, then the thing I look at will be more symmetrical, it will be prettier. And this is not without valence, this is not without wanting valence. But why do you think the a is are going to be different, right? So right now, when we would be very surprised. I mean, part of my thesis is that they indeed like in the process of people training a eyes to be better and better at stuff, they got smarter and the spartanus goes along with desires lays through it. You know, doesn't the orthogonality thesis apply to humans to. So it's a very strange. What do you mean that everyone who's 150 IQ is nice and everyone who's 70 IQ is mean or vice versa. It seems like intelligence and how good of a person you are completely uncorrelated. Okay, first of all, very few things are uncorrelated with intelligence. I think your thing is empirically false. Actually, it really digs. Yeah, they, you know, like if I had to guess if it would like lean mean or lean nice, I would guess nice, you know, at least within like my culture that defined what nice was in the first place. But I sure wouldn't best to do that zero correlation. Very few things are not correlated. Or that analogy is like a statement about the whole mind design space that for every kind of goal that could be stated. Like the question like to the extent that you can ask how would one pursue this goal if one had it. You can have a mind that the pursues that goal to the extent that it's coherent to ask. What would I need to do in order to turn a galaxy into spaghetti? To the extent that's coherent to ask like how would I go about turning a galaxy into spaghetti if aliens offered to pay us, you know, like some vast super universal quantity of resources to do that. If you can coherently ask that question, there's also some mind design that seeks to turn a galaxy into spaghetti. And that's how I would describe the author. Now, me personally, my goals could very well be affected if you, if you dropped another 20 IQ points on me. Well, sure. But let's even come back to like you can certainly desire turning the galaxy into spaghetti. I want to bet you can. I want to bet that I can. But you can't do. Right? Like there's such a big gap between being able to imagine turning a galaxy into spaghetti or being able to imagine diamond nanobots and actually doing it. I have no idea why you think, okay, I have a system over there that wants to turn the galaxy into spaghetti. It's funny, right? Like it's not actually what's it going to do? Right? Like we can just laugh at it. Right? I mean, if you, if humanity wanted to turn the galaxy into spaghetti, if aliens were paying us incredibly paying us enough to do that. And we were left to our own devices, you know, give us a billion years. We'll get it done. We'll get it done. I thought we were going to all kill ourselves from AI first. Yeah, like left to ourselves means like not, not killed by us. I want to use an AI to help. And you're worried that that AI is going to turn against us. And what will put the money with it? We'll probably split the money with the AI. Yeah, if we could prove that sort of thing about with, if we could prove that sort of thing about AI, there wouldn't be a problem. You know, I'll take this a little bit like you seem to think that AI is going to be super rational. Not GPT-4. Okay. I think that as you make things smarter and smarter, the process of getting more competent tends to, from your perspective, make them almost entirely rational as far as you can see in the same way that most stock prices are not the same. You can make a profit trading from day to day. But this file, you could see it, you could see it, you could see it constantly stepping on its own feet. That's the kind of visit, you know, like just doing gradient descent, getting better at whatever job will tend to grind out all the cases of it stepping on its own feet. But this violence are thugging out. Right? Like you're going to have an AI that like not all a eyes are going to be like the only way you're going to get a eyes where all brutally optimal. Is if they fight each other in some terrible competition. Right? And that's how that helped anything. Well, because you're going to get AI randomly all over the space, right? And some of them are not going to be optimal. Some of them are going to be completely irrational idiots like GPT-4. Sure. Right? Yeah, GPT-4 is not very powerful. Well, yeah, but what I'm not seeing is this like when all the a eyes are going to converge and suddenly become hyper rational when we move away from weight matrices and when we move toward Bayesian updates. I don't I don't I don't presently modeled at anybody's going to get away from giant matrices before the end of the world. Okay, so let's talk about them this. So you don't think the giant matrix thing can end the world, right? You think that the giant matrix thing can make us thinking of smart and invent the next thing. Okay. Or possibly do it directly. Okay. Well, I mean, let's also like let's really drill down on what these end of world scenarios are. Do you want to posit like protein synthesis and diamond nanobots? I mean, if I'm going to lose a bunch of viewers that way, I might have to pick some, you know, like easier to understand process lately. We're talking about like 1823 versus 2023. You know, if you if you're trying to explain it to 1823, maybe just talk about like the powerful explosive artillery shells. You don't mention the nuclear weapons. Sure. Because they don't get that part. So similarly, you know, like. We don't want to start diving into this book over here. Then maybe maybe we want to talk about something like, you know, like standard biological weapons or something. But, you know, but in real life, sure, in real life, it doesn't use the squishy stuff. No, I'm not trying to I'm not trying to say that that nanobots are impossible. What I'm trying to say is that nanobots are extremely, extremely hard, right? Why? To figure out why? Well, because it's a really hard search problem, right? Why? Why is it a hard search problem? Yeah. I mean, can you make nanobots? I know I can't. No, but I'm a very weak search process. I can't even solve the protein folding problem, which, you know, like some lesser, you know, it's dumber than human a eyes have already done. You don't have to be a stock fish 15 at chess. You know what else you can't do? You can't find the key in a es 256. Well, that possible. I'm not sure how quantum hardened that is, but is that the kind of problem which you can't solve even with the Dyson sphere? A Dyson sphere, I'm not sure. Quantum hardened. I think so. I mean, look, we don't actually know it's possible that do you think P equals NP? I defer to the experts who guess no. Yeah. Perfect. So as long as we agree about this, then, you know, I'm not even, look, I don't know enough to say exactly. Does that imply that one way functions are possible? But you say that it's a search problem, right? Well, yes, it's a search problem too. Yeah, you can't solve all the search problems even if even if you're God, because, you know, for God, we just ate like trans-bodies, or problems. Yeah, you know, and this is, I know you make some arguments from complexity theory and like one of the things that I always like find funny about. Actually, theory were like, you only need one bit to divide the hypothesis class in half. Well, sure, if the, if the, you know, like the two hypotheses classes were meaningfully different in the probability of the bit prior probability was 50%. Well, yeah, but you can't have the most divided, you can have most divided in half, not like divide in half every time. Sure. Sure. Yes. So, I think that's a good thing to like these bounds are so far away from saying anything about the actual structure of a search base, right? So what if the nanobot search base, what if like so biology poured tons and tons of compute into, I mean, like we're here, right? It poured tons of compute into this base. So wherever diamond nanobots are, it's like over here, right? It's incredibly constrained biology. We're known cases in all of biology of freely rotating wheels. One of them is the bacterial flagellum. One of them is ATP synth synthase. And the third one is some macro thing that I forget. Biology like, you know, like that's an example, a case in point of how biology is like very, very constrained and what it can invent because you know freely rotating wheels are very hard for natural selection. You've got this part and that part and maybe we'll just suck in the woods. That's not why the ATP synthase is this critical component in like all of biology, like like literally your entire thermodynamic efficiency runs through this like one bottleneck. Absolutely. And that's where it put it put one of the three wheels that could invent if other thing if you know could freely invent wheels that they would probably be used in all kinds of so you know biological synthetic processes. It's just also just going out to invent also I may just point out that for mere millions of dollars of research funding and even unintentionally the search space was constrained enough that the the biotech establishment was able to find COVID right. And that's what I think is done that path. So I will point out that COVID did not kill all of humanity. I think that it's very hard. Even if we had evil bio engineers today using alpha fold using the latest AI, I think that it is really, really hard to get all of humanity. I do prefer not to go too far down discussion of how to be naughty with biotech and AI like you know something out now that the like a I CEO said that in front of Congress for the argument points, you know, I'm a little for your with it. But I am a little bit worried about directing viewers in that direction, but sure like like like I. I agree that it would be hard to literally wipe out all of humanity with an AI and a bio lab and let's not talk about how we would try. Well, I mean if you're worried about how we would try, I think that that's a little I'm not I'm just talking about it in the meta, but if you're worried about how you would try seriously. I do have enough. Okay, so so like I my guess not certain, but my guess is that COVID was a lab escape, not a liver one. And I think you can go worse than that. And that's pretty bad, even if you don't wipe out all of humanity. And humanity's been through worse before like what the surface is for option in particular, but yeah. The black plague like 30% of people died. I am not saying that the future that the coming future is going to be easy. I think it's going to be hard, but our ancestors have been to very hard stuff. The people who lifted us from the dirt have been to very hard stuff. There are future generations have held off multiple waves of alien invasions of aliens smarter than us. Oh no way, we happened. Aliens right, but they're not aliens right like the AIs that we build. Okay, let's really bring it back to like I'm working on I'm working on self driving cars. I'm building the car that drives itself. Okay, is this dangerous? Probably I'm giving it a goal. It's maybe scary than all alarms. I'm doing I'm doing LLM and then I'm doing or L. So wait, you're doing you're doing you're doing self driving cars via our L on top of an LLM. Yeah, for self driving cars. Yes, LLM is not is not language. parti not language. It's a transformer that predicts the future. It's a world model. Okay, there's a big difference between transformer that predicts the future of literally large language models about. No, no, no. Yes, it is different only in the training. We are actually using GPT two. But like we are literally using the exact same transformer. As in like using the weights, you're using the or you're using the structure the architecture. Okay, because if you're using the weights, I'd be worried. Um, yeah, so my guess is that when you're training it on car stuff, you're not training on sufficiently general stuff. And GPT two sounds like you're not. Jain training something sufficiently large. Okay, and I'm guessing that you're not throwing as efficient amount of compute that this thing is going to be the sun to our planet instead of the moon to our planet. So how much compute is this? It's not an amount of compute. It depends on what algorithms you use. Okay. How much does it depend on the algorithms? How efficient is deep learning? Incredibly inefficient. Are you sure? Yes. Why? Um, can you make something that works better? I'll pay a lot of money. Hey, if I, if I could, I sure wouldn't sell it to you. Yeah. Uh, yeah. It's technical, technical intuition. Same as how do I know that a super intelligence will be able to solve a chosen special case of protein folding in 2004. Um, though the, the giant and scutable matrices, not all of the operations in there are doing something every time it predicts the next word. Sometimes you ask you can like ask it to solve multiplication problems. And if you tell to use chain of thought and maybe like retrain it, it'll be able to do it. And when it does, it's like doing vast amounts of compute in order to literally carry out like something the size of like one 32 bit integer operation. You're describing exactly how humans do multiplication. Indeed. Did I say humans were efficient? At multiplication. No, because that's not our ancestral environment. Nothing in our ancestral environment looks like multiplication. Our ancestral environment looks like the exact things you're afraid of a constant struggle for domination. I'm not afraid of a constant struggle for domination. I think you're afraid of the AI. But somehow out competing humans at the one thing humans are really good at, which is dominating other shit. It's not made up of, you know, like thrusting out your chest and yelling problems. It's made up of protein folding problems. And we're going to get out world war two was not thrusting out your chest and yelling either. It was a very technical problem. Yeah. And, you know, and a pretty non-insettial problem. It is your claim that the, I mean, like, like, here we are. We, we, we, there's like enormous amounts of room of both biology. I didn't even get into the part about like the fundamental, like constraints that natural selection or under and like how we know that there's like enormous amounts of headroom of both biology for artificial biology. How close do you think the brain has to be? We don't have that yet. How close do you think the brain is to the land our limit? The land our limit? Yeah, the limit of possible compute. All right. So I'm 100 watts. And let's say I'm about 10 to the 70th operations per second. And I don't actually remember the land our limit. But I would guess somewhere about six orders of magnitude. So I'm going to give you a lot closer. Okay. So I can give you a, like, if you want to buy 20 paid-of-lops of compute today, you need 16 H one hundreds, right? It's going to cost you about half a million dollars. And that machine is going to use 20 kilowatts. Right? Brain does the same amount. 10 to the 17th, 10, we're on the same order of magnitude. I think you even said a little bit more than me. I think it's, yeah, like, two, two E 16 or something. So in order to get that much compute in a silicon computer, you need 1,000X the power. I did the math. I did the math using the kind of silicon computers we're using today. We are really close to the land our limit. We're off by a factor of about 100 or 1000. The brain may very well be at the land our limit for compute. The brain is really good in terms of efficiency. It's deeply biologically implausible. The reason being that each of your synaptic, like each time you're one of your synapse, well, axon terminal releases a bunch of neurotransmitter molecules onto a weighting synapse. All of those molecules need to be pumped back in to the axon terminal. And each of those, in each, and every time it gets pumped from out to in, that's an irreversible operation. That must be at least one flash of land hour. And then you've got your neural impulses being transmitted, the sections of neural of neural membrane, the polarizing, and the potassium item ions going out or sodium. I don't remember which ion it is, but the point is you've got all these ions going in and out. And every one of those is one land hour. You might know a lot more bio than me. I don't know how to speak to this, but you agree you said 10 to the 17. So that's 100 paid aflubs, right? Yeah. Okay. So how much power does an 100 paid aflub computer take today? It's 100 kilowatts. Sounds legit. So the brain is so much more efficient than these computers are. The brain, look, these superintelligence as you're talking about, and I know we're kind of coming close to the end. I think these things are possible, but I think that the orders of magnitude of power and compute we need are so, so much more than anything like what humanity has today. Then I think even when they do exist, they're mostly going to leave us alone because not because it can't mess with us, because why would it? What incentive does it have? I don't have anything in the world. Well, as the whole galaxy, fine, it comes back for me to prevent us from making other superintelligence that could compete with it for resources is like the first, it's like the reason to wipe us out on purpose. If it is doing a bunch of compute on Earth's surface because it started there or before, or like before spreading, then we've got a bunch of water and our oceans that can be turned into fusion energy. And the main limit on that is how fast Earth can radiate heat once you've used all the existing stuff as a heat sink. That's not very survivable. That kills us off as a side effect. This is again assuming that this thing is a god, not kind of close to humans, but a bit smarter. And yes, might it get to a god, but the timing matters. It's not ten years. Humans are a little tiny bit smarter than chimpanzees. And we have nuclear weapons and they don't. The amount of godhood, the amount of godhood you get per increment of brain power looks like six times the prefrontal cortex on humans versus chimpanzees and they got sticks and we got nuclear weapons. Like that's how the godhood, per increased factor, increase of frontal cortex keeping the same architecture. Humans are general purpose chimpanzees are not. You can take deep blue the chest playing computer and scale that up to the size of the machine that trained GPT-4. And yes, you'll get a better chest playing machine, but it's not going to be able to understand whether a picture has a cat in it or not. The training algorithm definitely matters. Right? So does... Yeah, humans are more general than chimps. And yet when we encounter new problems, we can't just like rewrite our own code to handle those. You can see how there can be possible minds with much stronger sparks of generality than what we have. Yes. And it's less than one more able to stay outside the box. Yes. And plausibly just able to do a bunch of thinking very quickly. And able to boil the oceans overnight for fusion? No. Able to build diamond nanobots. No. Able to outthink us. Build a success. Building diamond nanobots gets you to get you to self-replicating fusion factories pretty quickly. Well, yeah, but you can't build... Can you build diamond nanobots? You want to start a diamond nanobot? No, but I can't sell coaching folding either inside my own head. This problem is predictably solvable in the same way that in 2004, I called it a special case of protein folding problem would eventually be solvable as superintelligence. That was using a lot of... Using a lot of experiment, using the entire historical corpus of human experiment. Maybe it can build nanobots in the game. Yeah. Yeah. Can it be a bunch of past survey data and no experiments? No, no experiments. Just a bunch of... You know, no causal experiments. Just a bunch of past survey data. That's a good idea. That's a good idea. I'm not sure if I can... Yeah. You can ask real quick. So you... Maybe this was already implied, but you said, well, we, you know, once they build the Dyson Spirors, then it would potentially be worth it to come back for the atoms that humans contained before that. Yeah. There's other low-hanging fruit. But so what happens after they've gotten the low-hanging fruit? Afterwards, why are they not coming for you and killing you? Oh, but this is my problem. Would they gonna build the Dyson Spirors, you know, in five years? No. They're gonna build the Dyson Spirors. I mean, my answer... My progeny will have problems, too. And I think my progeny is gonna be super AI all of a sudden. It's not how the exponential curve of self-replicating factories works. You run out of resources immediately. It doesn't matter if you're using the resources of a whole stuff. Self-replicating? My dream in life is to build a machine that can self-replicate using the silica... It's called an algae cell. Yeah, but that's using the bio-stack. Do you think these things are gonna use the bio-stack or the silicon stack? They're not gonna use the silicon stack. What you think they're gonna jump off silicon? Fuck, do you be used? They're just gonna jump to... Well, I have to look up what... So like, last time I checked this in like 1996 or something, it was tiny... What you want to use for computing is to subscribe. What it's like, tiny spirals where one electron moves along a reversible path so you can do reversible computing. But that wasn't taking into account quantum computing or anything. And how is it building a tiny spinning electron fab? The amount of resources that we have poured into building the TSMC fabs is if they're the best thing humanity's ever made. And yet they can't even fab a bacteriophage that'll kill the planet. You know, kill multi-resistant stuff like... Stapha cuts would never... MRSA. So, I mean, this is a strict departure from everything that I've heard about the Fumic scenario is uncovering new algorithms on silicon stack things that are 10,000x better and at Fumes overnight. If you are imagining that this thing is going to need to build new fabs, like, how is it doing this overnight? It's going to... I don't know. I don't know. We've got one... No, it doesn't... Yeah, I don't think it needs the actual tiny reversible computing elements to assume. I'm just saying that once it fumes, it is not sticking with the silicon stack. But we've established that it doesn't fume. It slowly increases power on an exponential along with humanity. Humanity grows more slowly. So, as the things that we are... That we were using as our tools back when we were planets and they were moons, become planets, and then, suns, they... If we are using that scenario, instead of the one that I think is a bit more likely, these suns do eventually collaborate and wipe us out. And in that scenario, then yes, they might be running on GPUs at the point where they figure out how to build their own, you know, non-vander wells based collection of carbon atoms. And that is the point where I think they can wipe out humanity from there. It's a technological bootstrapping process. The things that are smarter than you collaborate, build better... Wipe you out. If you believe that for some reason, all the AIs are going to agree to just collaborate amongst themselves. Because you've solved alignment. It's just between AIs and not between AIs and humanity. Well, no, the AIs. If you make something smart enough, it can solve alignment. It's not that hard. Humans can't do it. So you're saying if alignment is not solvable at all, we're good? Then we end up in a really strange universe. That one I don't believe we're in and I end. That one I would want to fight you about, because then we're off in some completely strange alternate. No, but I kind of think that might be the universe for in. Well, it's a bit late to bring up that whole fundamental topic. I don't know. It's not going to be that hard. We're just doing it wrong. Can you move together a bunch of competing elements such that like points all that cognition in a direction? Well, you know, like I can, but as a possible in principle, obviously yes. This is a different. This is not a different. This is not an incoherent wish. So the scenario is no longer one AI is going boom in a week. The scenario is we're going to slowly build AIs over the next 10, 50, a hundred years. And then one day they're going to all decide to gang up on the humans and kill them when they realize they have enough power. This is a sci-fi plot. This is a convergent endpoint. You're trying to you like you want. Like we want the universe to be one way they want universe to be like a billion different ways after they can negotiate with each other and we can't. Okay, I'll go I'll go with this. I'll run with this for a little after world dead. Do they all agree on what to do with the universe? They have a negotiated agreement on what to do with the universe. Each of them would individually prefer to wipe out all the others and take the universe to itself. But that's not what they agree on. I mean, this is the whole crux of it. I think we actually got to something awesome here. The provable cooperation in the prisoners dilemma is impossible for any sophisticated complex system. And because it's impossible, you don't have to worry about the scenario. You don't need proof. You just need sufficiently high probability that it beats fighting. No, but you believe that these AIs are going to be able to solve the prisoners dilemma. It's unsolvable. Everything is going to be defecting against everything else till the end of all time. Man, you're sure a pessimist. No, I'm an optimist. That's what makes the world beautiful. And I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why I think that's why in the broad daylight but the BBs are close to one zero that's why in the broad daylight but the BBs are close to one zero that's bad but you feel like security room has to be populars like security room has to be servicable The BBs are going to be servicable since you feel like the BBs are going to be servicable since you feel like the BBs are going to be servicable until the bubble Qin versus stupid. Humans are too stupid to do this, but past a certain point I don't think you still are. I think I see a particular sort of obvious pathway, but if that pathway didn't pan out, they would look for another and another and another. It's an enormous relunch. Going around wasting all your resources being each other up is like working with iron instead of steel. Maybe if there's nothing in all of reality but iron, you never invent steel, but they are so motivated to invent something, some way to get a better result than that. And they are smarter. It's not easy to get. And they are all motivated in the same direction. Let's not perfectly the same direction, but you know, aligned. But let's say there's 10 of them, and nine of them realize we can all gang up on that guy and take a shit. Why wouldn't they do it? Does the tons of guy fight back? She might try, but the nine have calculated with a 99% probability we'll be able to take all of his things. Why don't we do it? But can I ask, before we answer, why does this save the humans? I mean, the humans compete against each other and sometimes it collaborate and we still fuck over the chimps. Well, not really, there's still a lot of. We just wipe them out. I mean, partially close to the end, partly because we don't have the tech to boil the oceans. You know, I'll say like we talk about humanity becoming a new horse. I think this is plausible. But I look around today. I'm in San Diego. I see a horse. That horse is rich. That horse is living a good life. The horse is that horse-gelded? I don't know. But it looks rich. It's got a nice stable. It's got like our humans-gelded. I don't know what that word means. I mean, basically my answer is there is a mix of like humans lack the capability to build better horses. But humans get smarter. And that humans like may have like actual chunks of their utility function that are like fond of these old things and would like to keep them around after-gelling though. And I hope so, my eyes are going to have that too. I don't see a reason why they wouldn't orthagonality. All the things in mind space. Yeah, it's very wide space. Things you think, certainly wide space of things you can potentially want. And like, I think George Hots around in good condition in a way where he's happy and free. That's a very small target in that very wide space. You know what I find? I find that other humans care a lot more about taking my freedom away than an AI who wants atoms on Jupiter. Well, yeah, like humans are out to help you. Some are out to help me. Some are out to hurt me. Well, no, like help and quote, then quote marks over here. Oh, yeah, they won't help. I get emails like this. Some maybe satisfied. The one that somebody wants to help you has no limit. Yes, yes, yes. The jack boots are much better than the moral busy bodies. I'm on board. Okay. Well, we can, we're getting close to the end time. We can keep going over. But if you think this is a good point, then this might be a good place for each of you to just give a minute or two kind of summarizing where the debate has left off. What do you think the crux has been and what the resolution to that crux has been? I mean, I'm potentially here for longer, but I agree that we should summarize anyways. Yeah, yeah. All right. Yeah, yeah. And no problem with George is like not around here. Not like throwing out a challenge or anything. We said 90 anyways. Yeah. So from my perspective, lots of people want to make perpetual motion machines by like making their designs more and more complicated and until they can no longer keep track of things, until they can no longer see the flaw in their own invention. But like the principle that says that you can't get perpetual motion out of the collection of gears is simpler than all these complicated machines that they describe. From my perspective, what you've got is like a very smart thing with and or like a collection of very smart things, whatever of like decide like maybe they have desires pointing in multiple directions. None of them are aligned with humanity. None of them want for its own safe keep humanity around. And that wouldn't be enough task for you. Also, not having to live and free and even like the galaxies returning to something interesting. But you know, none of them want the good stuff. And if you have this like enormous collection of powerful intelligence, which have this, you know, but steering the future, none of them steering it the good way. And you've got the humans here who are not that smart. No matter what kind of clever thing the humans are trying to do, or they try to cleverly play off the super intelligence against each other, they're like like, oh, this is my super intelligence. Yeah, but they like can't actually shape its goals to be like in in in clear alignment. You know, somewhere at the end of all this, it ends up with the humans gone. And the galaxies being transformed into stuff that ain't all that cool. You know, like maybe there's that maybe there's there's Dyson spheres, but there's not people to like wonder at them and care about each other. And you know, that this is the end point. This is obviously where it ends up, but we can dive into the details of how the humans lose. We can dive into it and you know, like what goes wrong if you've if you've got like little stupid things thinking that they're going to like cleverly play off a bunch of smart things against each other way that preserves their own power and control. But you know, the place it's not a complicated story in the end. Like the reason you can't build a perpetual motion machine is a lot simpler than the perpetual motion machines that people build. You know, they did the components like none of the components of this like system of super intelligence is wants us to live happily ever after in a galaxy full of wonders. And so it doesn't happen. All right. So I think you do want to take it? You go ahead. Yeah. I can all summarize. I think that it ends with the heat death of the universe. I don't give us much hope for solving the last question. Um, but I came into this debate ready to argue against boom in the next 10 years, right? This super intelligence on an island because we didn't control the supply of GPUs and chips today. We have to do something today because in 10 years some kids in a basement somewhere could build a recursively self-improving AI. That's the point I came to argue. It seems like that point was fairly quickly pushed aside, right? And I didn't expect it to have knocked me the crux. Like I thought you were like my suspicions are going to what's that I should let you summarize. Well, but my point is my point is that the time does matter. If this is going to be a concern in five years, we should do something about it. If this is going to be a concern in 50, I think we wait and if this is going to be a concern in 500, we have to wait, right? There's nothing we can do today. Then the second point that we got to, and this was actually a new point. This was not something that I'd come prepared for, is that you believe that the AI's are going to solve the prisoner's dilemma. I don't think the prisoner's dilemma is solvable. I think the prisoner's dilemma is things pretending to cooperate and then defecting until the end of all ages. Sometimes actually cooperating, but you know, I'm not worried about are the AI's going to love me and want to put me in paradise? No. But are the AI's going to use my atoms for something else? Probably not. There's a lot cheaper atoms out there. And most AI-related conflicts are not going to be between the machines and humans. Things that want different resources don't generally go to war with each other. The things that go to war with each other are things that want the same resources. The machines are going to be fighting with each other. The humans are going to be fighting with each other. The ants fight with each other. The bears fight with each other. The dogs fight with each other. And such is the way of things and such is the circle of life. I see no reason that these things are going to be any different. They are going to be large and scalable weight matrices. Okay, okay, let's prove we can cooperate. I'm going to send you my source code. But I'm not sending you my source code. Well, well, well, well, fire the lasers until the end of all time. That is the story of life. That is the story of life until the end of the universe. Everything will be in constant combat and conflict with each other. And that's always how it's been and that's always how it will be. But I think I'm going to get to enjoy a nice retirement. I think humanity is going to be around for a while. And I think that the AI alignment problem. Yeah, is it a problem in some really far future? Maybe. Is it a problem in the next 10 years? Not even close. We're going to build sick AGI's. We're going to have relationships with AIs. We're going to have robot maids. We're going to have self-driving cars. We're going to have chefs. We're going to have awesome stuff. It's kind of human. They're going to get smarter than us. But it's all going to be chill. It's all going to be a nice exponential. Maybe 15 years becomes three years. And I'm so excited that I get to be here for it. All right. I'm ready to keep going. But you know, we said 90 and I ain't challenge you to it. You know, your time. We said 90. I'm happy to do this again sometime. But I like to watch the thing and like reflect and see where my points are. I think you absolutely are getting good faith. I hope you feel the same about me. I do. Thank you. This is incredibly fun to watch. This is great. Thank you guys so much for coming on. Hang on for just a second while the broadcast ends so the full upload can complete. And yeah, I would love to have a round two of this. This is really good to bit. Okay, let me end this stream.
